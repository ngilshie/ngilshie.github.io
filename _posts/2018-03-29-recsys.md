---
layout: post
title:  "A Deep Learning-based Recommender System for AI Papers"
date:   2018-03-29 21:33:23 +0800
categories: jekyll update
published: true
---

# 1. Introduction

My friends over at <a href="https://nurture.ai/">nurture.ai</a> gave me the opportunity to build a recommender system for Artificial Intelligence papers, with the hope that such a thing may help AI researchers and enthusiasts alike navigate the vast amounts of AI literature that is already present, and that is still being churned out at lightning speed. As with all other recommender systems, the idea is to have AI papers of interest find the readers, instead of the other way round. You have probably interacted with recommender systems at some point in your life --- Amazon's "top picks," Youtube's "recommended videos for you," and Facebook's "suggested friends," just to name a few.

Thinking that nothing else could be more meta (think creating AI for AI research), I spent a month or so brushing up on natural language processing (NLP) and implementing a prototype. It's a prototype because it leaves out many of the essential features of recommender systems like utility matrices and item profiles, and instead focuses on getting the core NLP and deep learning algorithms working. In this blogpost, I describe my implementation of the four main parts to this prototype:

1. **GloVe word embeddings** --- I train GloVe word embeddings on a corpus of 10,000 AI papers pulled from <a href="https://arxiv.org/">arXiv</a>. These word embeddings are a pre-requisite for any NLP technique that uses deep learning.
2. **Latent Dirichlet Allocation (LDA)** --- I perform topic modelling using LDA, to generate the weak supervision labels needed for Part 3.
3. **Long short-term memory (LSTM) network** --- I use a single-layer LSTM network to obtain vectorized feature representations for each paper's abstract, encoded by the hidden layer units of the LSTM. These feature representations can be used to measure similarity between papers.
4. **k-nearest neighbour (knn) search** --- I perform simple knn search on the feature representations from Part 3 to find similar papers and hence make recommendations.
 
---

<br>
  
# 2. GloVe word embeddings
If you're familiar with Word2Vec models like <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Skip-Gram</a>, GloVe (Global Vectors) is similar in that it generates word embeddings in the form of vectors, typically 200 or 300-dimensional ones, depending on the application. GloVe differs from Word2Vec mainly in the supervision signal that is being used during training --- whereas Word2Vec models try to tune word vectors to accurately predict a context word within some sliding window, GloVe trains word vectors to accurately predict **word co-occurrence counts** within the entire corpus (hence the qualification "global"). This is formalized as a weighted least squares regression problem with the loss function

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^V\sum_{j=1}^V f(X_{ij})(u_i^Tv_j - \log X_{ij})^2,
$$

where 

- $V$ is the vocabulary size,
- $X_{ij}$ is the number of times that context word $j$ co-occurs with center word $i$,
- $f$ is a weighting function,
- $u_{i}$ is the word vector of the center word $i$,
- $v_{j}$ is the word vector of the context word $j$, and
- $\theta$ is the vector of all model weights that need to be trained, which, in the simplest case, is just the concatenation of the flattened matrix $\mathbf{U}$ of all center word vectors and the flattened matrix $\mathbf{V}$ of all context word vectors.

Without taking derivatives, we can see intuitively from the loss function that the model would achieve perfect fit if $u_i^Tv_j = \log X_{ij}$, that is, when the dot product of the center and context word vectors equals the log of the co-occurrence count for the same pair of words. The weighting function $f$ takes the form

$$
f(x) = \begin{cases} (x/x_{max})^\alpha &\text{ if } x < x_{max} \\ 1 &\text{ otherwise, } \end{cases}
$$

which when plotted with $x_{max}=1$ and $\alpha=0.75$, looks like this:

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 1: GloVe model weighting function $f$</font></figcaption>
	<img src = "\assets\weight_func.png" height = "200" width = "400" align = "middle">
</figure>

This form for $f$ confers several benefits, the most important of which is that the loss contributed by more frequent co-occurrences is weighted more heavily, but that beyond some threshold $x_{max}$, the weights are capped. Training involves minimizing the loss function $J(\theta)$ with stochastic gradient descent or other similar optimization methods, and the trained word vectors can be obtained by taking the element-wise sum of $\mathbf{U}$ and $\mathbf{V}$.semantic vs syntactic

For the training corpus, I followed the steps in Andrej Karpathy's arXiv Sanity Preserver <a href="https://github.com/karpathy/arxiv-sanity-preserver">GitHub repo</a> to obtain 10,000 AI-related papers from arXiv in .txt format. I then used StanfordNLP's <a href="https://nlp.stanford.edu/software/tokenizer.shtml">PTBTokenizer</a> to generate white-spaced-separated tokens from these 10,000 papers, and followed the instructions at StanfordNLP's GloVe <a href="https://github.com/stanfordnlp/GloVe/tree/master/src">GitHub repo</a> to train my word embeddings. For training parameters, I adapted the setup that was presented in the original 2014 <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe paper</a> by Pennington, Socher & Manning, and used

- an embedding size of 200 (i.e. to generate 200-dimensional word vectors),
- 50 iterations for the training,
- $x_{max} = 100$ and $\alpha = 0.75$,
- a window size of 15, and
- a minimum count of 5 for any given word to be included in the corpus vocabulary.

Training took about 80 minutes on my Intel Core i7 processor (to my knowledge, StanfordNLP's implementation of GloVe doesn't use GPU). The total vocabulary size of words that appear at least 5 times in the corpus is 209,126. Now, for some exploratory data analysis. Figure 2 below shows the counts for the ten most frequently-occurring tokens that aren't digits, punctuation, stop-words, or other gibberish.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 2: Ten most frequently-occurring (meaningful) tokens</font></figcaption>
	<img src = "\assets\counts.png" height = "230" width = "700" align = "middle">
</figure>

As might be expected of AI papers, words like *model*, *learning*, and *data* appear very frequently. It's instructive to note that *using* and *used* are treated as different tokens --- although it's possible for the human to clean the corpus by lemmatizing all tokens (that is, mapping *using*, *used*, *uses*, *etc* to their root form *use*), the GloVe model, as you shall see in a moment, is able to recognize the **semantic similarities** among these different forms. Besides saving the human hours of data-cleaning effort each time the corpus expands, retaining these different unlemmatized forms also preserves their **syntactic differences**, which turns out to be beneficial when using neural language models that rely on the linear order of words to extract meaning.

To illustrate these ideas, I perform $t$-distributed stochastic neighbor embedding (<a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">$t$-SNE</a>), a form a dimensionality reduction, to map the 200-dimensional word embeddings of the 1000 most frequent tokens onto a 2-dimensional space. Figure 3 below shows the resulting visualization.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 3: $t$-SNE plot of 1000 most frequent tokens</font></figcaption>
	<img src = "\assets\tsne.png" height = "400" width = "650" align = "middle">
</figure>

This is quite a mess, so let's break our analysis down into three regions. Figure 4 below shows Region 1 of the original $t$-SNE plot. In the original plot, this region seems to form its own distinct cluster, and looking at Figure 4, we see why. The majority of tokens in this region isn't really English words, and is stuff that one might remove during the data-cleaning or tokenization process. Nonetheless, the GloVe model is able to make some sense of these tokens. We see a distinct cluster that comprises some of the most common Chinese last names and what are presumably English middle names. Another distinct cluster contains calendar years for, perhaps, the published papers themselves. Yet another cluster contains seemingly random numbers that are obviously not calendar years. Interestingly enough, the spatial distribution of these numbers seem to reflect the different magnitudes --- single-digit numbers have been pushed to the right, while numbers in the hundreds or thousands are further down.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 4: Region 1 of $t$-SNE plot</font></figcaption>
	<img src = "\assets\tsne1.png" height = "400" width = "600" align = "middle">
</figure>

Figure 5 below shows Region 2 of the original $t$-SNE plot. Here, we see more familiar-looking tokens, in the form of English words. Immediately obvious is how certain words with nearly-identical spelling almost overlap each other: *architecture* and *architectures*, *model* and *models*, *approximate* and *approximation*, *etc*. The very short distances between the members in these word-pairs reflect their high **semantic similarity**. Yet, these distances are non-zero because of certain **syntactic differences** that are perhaps encoded in the 198 dimensions that this $t$-SNE visualization has collapsed. For example, though *approximate* and *approximation* bear the same meaning, the former is used as a verb and the latter, as a noun. Other obvious word-pairs include *LSTM* and *RNN*, *2D* and *3D*, and *adversarial* and *generative*, which are all words that tend to co-occur. 

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 5: Region 2 of $t$-SNE plot</font></figcaption>
	<img src = "\assets\tsne2.png" height = "400" width = "600" align = "middle">
</figure>

Figure 6 below shows Region 3 of the original $t$-SNE plot. Here, the plotted points are much more spaced out, with less obvious clustering. Inspecting the tokens reveals why: most of these words are generic stop-words that are used all throughout the corpus and hence co-occur with many other words. If you're wondering why I didn't remove from the corpus these words, along with those in Region 1, you are absolutely justified in doing so. For the specific task of measuring similarity between papers, these non-content words should probably be left out. Nonetheless, because the resulting vocabulary wasn't so large as to cause memory issues, I have left these words in for this first prototype.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 6: Region 3 of $t$-SNE plot</font></figcaption>
	<img src = "\assets\tsne3.png" height = "400" width = "600" align = "middle">
</figure>

---

<br>




