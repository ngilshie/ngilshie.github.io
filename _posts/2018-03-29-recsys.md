---
layout: post
title:  "A Deep Learning-based Recommender System for Artificial Intelligence Papers"
date:   2018-03-29 21:33:23 +0800
categories: jekyll update
published: true
---

# 0. Introduction

My friends over at <a href="https://nurture.ai/">nurture.ai</a> gave me the opportunity to build a recommender system for Artificial Intelligence papers, with the hope that such a thing may help AI researchers and enthusiasts alike navigate the vast amounts of AI literature that is already present, and that is still being churned out at lightning speed. As with all other recommender systems, the idea is to have AI papers of interest find the readers, instead of the other way round. You have probably interacted with recommender systems at some point in your life --- Amazon's "top picks," Youtube's "recommended videos for you," and Facebook's "suggested friends," just to name a few.

Thinking that nothing else could be more meta (think creating AI for AI research), I spent a month or so brushing up on natural language processing (NLP) and implementing a prototype. It's a prototype because it leaves out many of the essential features of recommender systems like utility matrices and item profiles, and instead focuses on getting the core NLP and deep learning algorithms working. In this blogpost, I describe my implementation of the four main parts to this prototype:

1. **GloVe word embeddings** --- I train GloVe word embeddings on a corpus of 10,000 AI papers pulled from <a href="https://arxiv.org/">arXiv</a>. These word embeddings are a pre-requisite for any NLP technique that uses deep learning.
2. **Latent Dirichlet Allocation (LDA)** --- I perform topic modelling using LDA, to generate the weak supervision labels needed for Part 3.
3. **Long short-term memory (LSTM) network** --- I use a single-layer LSTM network to obtain vectorized feature representations for each paper's abstract, encoded by the hidden layer units of the LSTM. These feature representations can be used to measure similarity between papers.
4. **k-nearest neighbour (knn) search** --- I perform simple knn search on the feature representations from Part 3 to find similar papers and hence make recommendations.
 
---

<br>
  
# 1. GloVe word embeddings
If you're familiar with Word2Vec models like <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Skip-Gram</a>, GloVe (Global Vectors) is similar in that it generates word embeddings in the form of vectors, typically 200 or 300-dimensional ones, depending on the application. GloVe differs from Word2Vec mainly in the supervision signal that is being used during training --- whereas Word2Vec models try to tune word vectors to accurately predict a context word within some sliding window, GloVe trains word vectors to accurately predict **word co-occurrence counts** within the entire corpus (hence the qualification "global"). This is formalized as a weighted least squares regression problem with the loss function

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^V\sum_{j=1}^V f(X_{ij})(u_i^Tv_j - \log X_{ij})^2,
$$

where 

- $V$ is the vocabulary size,
- $X_{ij}$ is the number of times that context word $j$ co-occurs with center word $i$,
- $f$ is a weighting function,
- $u_{i}$ is the word vector of the center word $i$,
- $v_{j}$ is the word vector of the context word $j$, and
- $\theta$ is the vector of all model weights that need to be trained, which, in the simplest case, is just the concatenation of the flattened matrix $\mathbf{U}$ of all center word vectors and the flattened matrix $\mathbf{V}$ of all context word vectors.

Without taking derivatives, we can see intuitively from the loss function that the model would achieve perfect fit if $u_i^Tv_j = \log X_{ij}$, that is, when the dot product of the center and context word vectors equals the log of the co-occurrence count for the same pair of words. The weighting function $f$ takes the form

$$
f(x) = \begin{cases} (x/x_{max})^\alpha &\text{ if } x < x_{max} \\ 1 &\text{ otherwise, } \end{cases}
$$

which when plotted with $x_{max}=1$ and $\alpha=0.75$, looks like this:

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 1: GloVe model weighting function $f$</font></figcaption>
	<img src = "\assets\weight_func.png" height = "200" width = "400" align = "middle">
</figure>

This form for $f$ confers several benefits, the most important of which is that the loss contributed by more frequent co-occurrences is weighted more heavily, but that beyond some threshold $x_{max}$, the weights are capped. Training involves minimizing the loss function $J(\theta)$ with stochastic gradient descent or other similar optimization methods, and the trained word vectors can be obtained by taking the element-wise sum of $\mathbf{U}$ and $\mathbf{V}$.semantic vs syntactic

For the training corpus, I followed the steps in Andrej Karpathy's arXiv Sanity Preserver <a href="https://github.com/karpathy/arxiv-sanity-preserver">GitHub repo</a> to obtain 10,000 AI-related papers from <a href="https://arxiv.org/">arXiv</a> in .txt format. I then used StanfordNLP's <a href="https://nlp.stanford.edu/software/tokenizer.shtml">PTBTokenizer</a> to generate white-spaced-separated tokens from these 10,000 papers, and followed the instructions at StanfordNLP's GloVe <a href="https://github.com/stanfordnlp/GloVe/tree/master/src">GitHub repo</a> to train my word embeddings. For training parameters, I adapted the setup that was presented in the original 2014 <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe paper</a> by Pennington, Socher & Manning, and used

- an embedding size of 200 (i.e. to generate 200-dimensional word vectors),
- 50 iterations for the training,
- $x_{max} = 100$ and $\alpha = 0.75$,
- a window size of 15, and
- a minimum count of 5 for any given word to be included in the corpus vocabulary.

Training took about 80 minutes on my Intel Core i7 processor (to my knowledge, StanfordNLP's implementation of GloVe doesn't use GPU). The total vocabulary size of words that appear at least 5 times in the corpus is 209,126. Now, for some exploratory data analysis. Figure 2 below shows the counts for the ten most frequently-occurring tokens that aren't digits, punctuation, stop-words, or other gibberish.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 2: Ten most frequently-occurring (meaningful) tokens</font></figcaption>
	<img src = "\assets\counts.png" height = "230" width = "700" align = "middle">
</figure>

As might be expected of AI papers, words like *model*, *learning*, and *data* appear very frequently. It's instructive to note that *using* and *used* are treated as different tokens --- although it's possible for the human to clean the corpus by lemmatizing all tokens (that is, mapping *using*, *used*, *uses*, *etc* to their root form *use*), the GloVe model, as you shall see in a moment, is able to recognize the **semantic similarities** among these different forms. Besides saving the human hours of data-cleaning effort each time the corpus expands, retaining these different unlemmatized forms also preserves their **syntactic differences**, which turns out to be beneficial when using neural language models that rely on the linear order of words to extract meaning.

To illustrate these ideas, I perform $t$-distributed stochastic neighbor embedding (<a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">$t$-SNE</a>), a form a dimensionality reduction, to map the 200-dimensional word embeddings of the 1000 most frequent tokens onto a 2-dimensional space. Figure 3 below shows the resulting visualization.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 3: $t$-SNE plot of 1000 most frequent tokens</font></figcaption>
	<img src = "\assets\tsne.png" height = "400" width = "650" align = "middle">
</figure>

This is quite a mess, so let's break our analysis down into three regions. Figure 4 below shows Region 1 of the original $t$-SNE plot. In the original plot, this region seems to form its own distinct cluster, and looking at Figure 4, we see why. The majority of tokens in this region isn't really English words, and is stuff that one might remove during the data-cleaning or tokenization process. Nonetheless, the GloVe model is able to make some sense of these tokens. We see a distinct cluster that comprises some of the most common Chinese last names and what are presumably English middle names. Another distinct cluster contains calendar years for, perhaps, the published papers themselves. Yet another cluster contains seemingly random numbers that are obviously not calendar years. Interestingly enough, the spatial distribution of these numbers seem to reflect the different magnitudes --- single-digit numbers have been pushed to the right, while numbers in the hundreds or thousands are further down.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 4: Region 1 of $t$-SNE plot</font></figcaption>
	<img src = "\assets\tsne1.png" height = "400" width = "600" align = "middle">
</figure>

Figure 5 below shows Region 2 of the original $t$-SNE plot. Here, we see more familiar-looking tokens, in the form of English words. Immediately obvious is how certain words with nearly-identical spelling almost overlap each other: *architecture* and *architectures*, *model* and *models*, *approximate* and *approximation*, *etc*. The very short distances between the members in these word-pairs reflect their high **semantic similarity**. Yet, these distances are non-zero because of certain **syntactic differences** that are perhaps encoded in the 198 dimensions that this $t$-SNE visualization has collapsed. For example, though *approximate* and *approximation* bear the same meaning, the former is used as a verb and the latter, as a noun. Other obvious word-pairs include *LSTM* and *RNN*, *2D* and *3D*, and *adversarial* and *generative*, which are all words that tend to co-occur. 

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 5: Region 2 of $t$-SNE plot</font></figcaption>
	<img src = "\assets\tsne2.png" height = "400" width = "600" align = "middle">
</figure>

Figure 6 below shows Region 3 of the original $t$-SNE plot. Here, the plotted points are much more spaced out, with less obvious clustering. Inspecting the tokens reveals why: most of these words are generic stop-words that are used all throughout the corpus and hence co-occur with many other words. If you're wondering why I didn't remove from the corpus these words, along with those in Region 1, you are absolutely justified in doing so. For the specific task of measuring similarity between papers, these non-content words should probably be left out. Nonetheless, because the resulting vocabulary wasn't so large as to cause memory issues, I have left these words in for this first prototype.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 6: Region 3 of $t$-SNE plot</font></figcaption>
	<img src = "\assets\tsne3.png" height = "400" width = "600" align = "middle">
</figure>

---

<br>

# 2. Latent Dirichlet Allocation (LDA)

For the whole recurrent neural network (RNN) idea to work, we require a training label for each of the 10,000 papers in our corpus. Hand-labelling these papers individually would be exhausting, and to my knowledge there is no convenient way to extract a uniform set of keywords, tags, or topics from the papers at arXiv. This is where an unsupervised topic modelling technique like Latent Dirichlet Allocation (LDA) comes in --- to generate the "ground truth" training labels for what is called *weakly*-supervised learning on the RNN's part. This may sound questionable, but has been studied to some extent in <a href="https://academic.oup.com/nsr/article-pdf/5/1/44/24164438/nwx106.pdf">literature</a>, and is the method I have relied on for this prototype, in the absence of better alternatives.

With that out of the way, let us briefly recap LDA. LDA has come a long way since <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Andrew Ng's paper</a> in 2003, and despite the myriad generalizations and extensions, remains highly popular as a topic modelling technique today. If you know the theory behind LDA, then you should be familiar with the diagram below.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 7: Graphical representation of LDA</font></figcaption>
	<img src = "\assets\LDA.jpg" height = "200" width = "400" align = "middle">
</figure>

The above figure depicts the generative process for the observed words in a corpus of documents. Before describing this process, let's define all of the variables in Figure 7:

- $M$ and $K$ represent the number of documents and topics respectively in the corpus
- $N_m$ is a random variable that denotes the number of words in document $m$, for some $$m\in\{1,2, ... , M\}$$
- The dashed rectangles are "replicate plates" that each represents repeated samples of either documents, topics, or words (any variable inside a rectangle implicitly gains the respective index $n, m,$ or $k$)
- $\theta_m$ is the topic distribution for document $m$, and is parameterized by $\alpha$
- $\varphi_k$ is the word distribution for topic $k$, and is parameterized by $\beta$
- $z_{mn}$ is the unobserved (i.e. latent) topic for the $n$th word of document $m$
- $w_{mn}$ is the observed $n$th word of document $m$

It should be obvious that the words $w_{mn}$ are the only observable variables in the whole model, with the others being unobservable i.e. latent, hence the name of the model. The generative process for an observed word $w_{mn}$, then, is as follows:

1. For each document $m$, choose a topic distribution $\theta_m \sim \text{Dir}(\alpha)$, where $\text{Dir}(\alpha)$ is a <a href="http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/">Dirichlet distribution</a> with a vector $\alpha$ of concentration parameters.
2. For each topic $k$, choose a word distribution $\varphi_k \sim \text{Dir}(\beta)$, where $\beta$ is another vector of concentration parameters.
3. For each word position $n$ in document $m$,
   - choose a topic $z_{mn} \sim \text{Multinomial}(\theta_m)$, then
   - choose a word $w_{mn} \sim \text{Multinomial}(\varphi_{z_{mn}})$.

Given the above generative process, LDA is then concerned with the inferential problem of computing the posterior distribution of the latent variables given the observed ones and other preset parameters, that is, $P(\theta, \varphi, \mathbf{z} \| \mathbf{w}, \alpha, \beta)$. But, that's enough for a recap, and we will not dive into the estimation details.

To apply LDA to the task at hand, I let each document of the corpus be a paper abstract, instead of the entire paper. Doing so brings the advantage of speed without compromising too much statistical power (assuming that abstracts tend to be at least as content-rich as the papers they summarize; of course, nothing is stopping you from running LDA on a corpus of full-length papers). With 10,000 tokenized abstracts, $M=10000$, and $N_m$ varies between 37 and 593. For this prototype, I chose $K=20$ topics, and used the Python package `gensim`. Listing 1 below shows the output word distributions ($\varphi_k$) for some of the topics.

<center>Listing 1: Truncated word distributions for select topics</center>
```
Topic 1: 0.021*face + 0.010*facial + 0.009*images + 0.007*network + 0.006*detection + 0.006*model + 0.005*recognition + ...
Topic 2: 0.008*learning + 0.007*agents + 0.006*agent + 0.006*show + 0.005*performance + 0.005*model + 0.005*human + ...
Topic 3: 0.012*attention + 0.011*question + 0.011*speech + 0.010*model + 0.007*problem + 0.006*show + 0.006*answering + ...
   :
   :
Topic 20: 0.028*image + 0.012*images + 0.008*attributes + 0.006*proposed + 0.006*ontology + 0.005*attribute + 0.004*color + ...
```
Each line shows a word distribution for a topic, in which the coefficients represent the probability of the respective word appearing, given the topic (all coefficients in the same equation should therefore sum to 1). Inspecting the word distribution for Topic 1, we see that Topic 1 has something to do with image or facial recognition using neural networks. Meanwhile, Topic 2 is probably reinforcement learning, and Topic 3 seems to concern speech question and answering using RNNs with attention. The output contains some ambiguity, however, because the last topic also concerns image recognition, which overlaps to some extent with Topic 1. Given more time, we should probably fine-tune the value of $K$ to generate more mutually exclusive topics.

Next, we look at the output topic distributions ($\theta_m$) for some of the abstracts, shown in Listing 2 below.

<center>Listing 2: Truncated topic distributions for the first five abstracts</center>
```
Abstract 1: 0.677*Topic1 + 0.314*Topic7 + ...
Abstract 2: 0.700*Topic1 + 0.033*Topic7 + 0.117*Topic15 + 0.028*Topic17 + 0.114*Topic20 + ...
Abstract 3: 0.051*Topic8 + 0.142*Topic13 + 0.092*Topic15 + 0.706*Topic17 + ...
Abstract 4: 0.035*Topic14 + 0.955*Topic15 + ...
Abstract 5: 0.017*Topic3 + 0.023*Topic5 + 0.468*Topic7 + 0.260*Topic13 + 0.026*Topic15 + ...
   :
   :
```
As with the word distributions, each line shows the probability that an abstract belongs to the respective topic that follows the coefficient. The first abstract is a mixture of Topics 1 and 7, while the second abstract is predominantly a mixture of Topics 1, 15, and 20. Topics 1 and 20, as shown above, concern image recognition. Topic 7 is a pretty generic topic about model training methods, while Topic 15 is loosely about unsupervised learning using generative adversarial networks. It turns out that the title of Abstract 1 was 

>Accurate Facial Parts Localization and Deep Learning for 3D Facial Expression Recognition

while that for Abstract 2 was

>Spectral Image Visualization Using Generative Adversarial Networks

Pretty cool!

---

<br>

# 3. Long short-term memory (LSTM) network

At long last, we arrive at the workhorse of this prototype, the part that makes this recommender system deep learning-based. In a sense, everything in this article so far has been the setup for this. If you aren't familiar with long short-term memory (LSTM) networks, I highly recommend this <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">amazing article</a> by Christopher Olah (even if you're already familiar with LSTMs, I highly recommend reading it). Otherwise, what follows is a (very) quick review of LSTMs, before we see its application to our problem.

An LSTM network is essentially a recurrent neural network with so-called *long short-term memory* units. We can explain this funny-sounding name through a set of equations, so let's pin down a few notational details first. Figure 8 below depicts a single LSTM cell.

<figure>
	<figcaption align = "middle"><font size = "4">Fig. 8: An LSTM cell (drawing inspired by Chris Olah's blog)</font></figcaption>
	<img src = "\assets\LSTM.png" height = "350" width = "700" align = "middle">
</figure>

With reference to the above diagram, let

- $h_{t-1}$ be the hidden state vector at time $t-1$,
- $c_{t-1}$ be the cell state vector at time $t-1$, 
- $x_t$ be the input word embedding at time $t$,
- $W^{(\cdot)}, U^{(\cdot)}$ be weight matrices for the respective operations,
- $$\|, *,$$ and $+$ be concatenation, element-wise multiplication, and element-wise addition respectively,
- $\sigma$ and $\tanh$ be the element-wise sigmoid and hyperbolic tangent activation functions respectively.

Then, a single LSTM cell at time $t$ computes the following quantities:

$$
\begin{align*}
f_t &= \sigma\left(W^{(f)}x_t + U^{(f)}h_{t-1} \right) &&\text{the forget gate} \\
i_t &= \sigma\left(W^{(i)}x_t + U^{(i)}h_{t-1} \right) &&\text{the input gate} \\
\tilde{c}_t &= \tanh\left(W^{(\tilde{c})}x_t + U^{(\tilde{c})}h_{t-1} \right) &&\text{candidate cell state values} \\
o_t &= \sigma\left(W^{(o)}x_t + U^{(o)}h_{t-1} \right) &&\text{the output gate} \\
c_t &= f_t * c_{t-1} + i_t * \tilde{c}_t &&\text{updated cell state} \\
h_t &= o_t * \tanh(c_t) &&\text{updated hidden state} \\
\end{align*}
$$

To better understand these equations, you could try tracing the flow of information in Figure 8. Intuitively, the *forget gate* $(f_t)$ controls how much information from the previous cell state $(c_{t-1})$ is forgotten, the *input gate* $(i_t)$ controls how much new information (in the form of the candidate cell state values $\tilde{c}_t$) should flow into the updated cell state $(c_t)$, and the *output gate* $(o_t)$ controls how much information in the updated cell state $(c_t)$ should flow into the updated hidden state $(h_t)$. The updated cell and hidden states ($c_t$ and $h_t$) are then passed on to the next LSTM cell, which will recursively compute the same set of equations above.

The trainable weights in the LSTM cell are the matrices $W^{(\cdot)}$ and $U^{(\cdot)}$ marked in red in Figure 8. All LSTM cells in the same network use these same weight matrices, and training involves passing error signals (i.e. gradients) from later to earlier cells to update these weight matrices --- a process called **backpropagation through time**. The name *long short-term memory* reflects the network's ability to selectively remember or forget the information it progressively extracts from the linear order of words. These LSTM cells collectively store *short-term memories* of the information they have extracted, potentially over *long* time horizons. For a deeper discussion of the kinds of long-term dependencies that LSTMs can model, take a look at this <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">fantastic article</a> by Andrej Karpathy.












