---
layout: post
title:  "A Deep Learning-based Recommender System for AI Papers"
date:   2018-03-29 21:33:23 +0800
categories: jekyll update
published: true
---

# 1. Introduction

My friends over at <a href="https://nurture.ai/">nurture.ai</a> gave me the opportunity to build a recommender system for Artificial Intelligence papers, with the hope that such a thing may help AI researchers and enthusiasts alike navigate the vast amounts of AI literature that is already present, and that is still being churned out at lightning speed. As with all other recommender systems, the idea is to have AI papers of interest find the readers, instead of the other way round. You have probably interacted with recommender systems at some point in your life --- Amazon's "top picks," Youtube's "recommended videos for you," and Facebook's "suggested friends," just to name a few.

Thinking that nothing else could be more meta (think creating AI for AI research), I spent a month or so brushing up on natural language processing (NLP) and implementing a prototype. It's a prototype because it leaves out many of the essential features of recommender systems like utility matrices and item profiles, and instead focuses on getting the core NLP and deep learning algorithms working. In this blogpost, I describe my implementation of the four main parts to this prototype:

1. **GloVe word embeddings** --- I train GloVe word embeddings on a corpus of 10,000 AI papers pulled from <a href="https://arxiv.org/">arXiv</a>. These word embeddings are a pre-requisite for any NLP technique that uses deep learning.
2. **Latent Dirichlet Allocation (LDA)** --- I perform topic modelling using LDA, to generate the weak supervision labels needed for Part 3.
3. **Long short-term memory (LSTM) network** --- I use a single-layer LSTM network to obtain vectorized feature representations for each paper's abstract, encoded by the hidden layer units of the LSTM. These feature representations can be used to measure similarity between papers.
4. **k-nearest neighbour (knn) search** --- I perform simple knn search on the feature representations from Part 3 to find similar papers and hence make recommendations.
 
---

<br>
  
# 2. GloVe word embeddings

If you're familiar with Word2Vec models like <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Skip-Gram</a>, GloVe is similar in that it generates word embeddings in the form of vectors, typically 200 or 300-dimensional ones, depending on the application. GloVe differs from Word2Vec mainly in the supervision signal that is being used during training --- whereas Word2Vec models try to tune word vectors to accurately predict a context word within some sliding window, GloVe trains word vectors to accurately predict **word co-occurrence counts** within the entire corpus. This is formalized as a weighted least squares regression problem with the loss function

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^V\sum_{j=1}^V f(X_{ij})(u_i^Tv_j - \log X_{ij})^2,
$$

where 

- $V$ is the vocabulary size,
- $X_{ij}$ is the number of times that context word $j$ co-occurs with center word $i$,
- $f$ is a weighting function,
- $u_{i}$ is the word vector of the center word $i$,
- $v_{j}$ is the word vector of the context word $j$, and
- $\theta$ is the vector of all model weights that need to be trained, which, in the simplest case, is just the concatenation of the flattened matrix $\mathbf{U}$ of all center word vectors and the flattened matrix $\mathbf{V}$ of all context word vectors.

Without taking derivatives, we can see intuitively from the loss function that the model would achieve perfect fit if $u_i^Tv_j = \log X_{ij}$, that is, when the dot product of the center and context word vectors equals the log of the co-occurrence count for the same pair of words. The weighting function $f$ takes the form

$$
f(x) = \begin{cases} (x/x_{max})^\alpha &\text{ if } x < x_{max} \\ 1 &\text{ otherwise, } \end{cases}
$$

which when plotted with $x_{max}=1$ and $\alpha=0.75$, looks like this:

<img src = "\assets\weight_func.png" height = "200" width = "400" align = "middle">

This form for $f$ confers several benefits, the most important of which is that the loss contributed by more frequent co-occurrences is weighted more heavily, but that beyond some threshold $x_{max}$, the weights are capped. Training involves minimizing the loss function $J(\theta)$ with stochastic gradient descent or other similar optimization methods, and the trained word vectors can be obtained by taking the element-wise sum of $\mathbf{U}$ and $\mathbf{V}$.






